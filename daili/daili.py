#!/usr/bin/env python3
"""
GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•è„šæœ¬ - æ”¯æŒHTTPã€HTTPSã€SOCKS4ã€SOCKS5ä»£ç†æµ‹è¯•
"""

import os
import sys
import time
import json
import socket
import requests
import concurrent.futures
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import threading
import re
from urllib.parse import urlparse
import warnings

class GitHubProxyTester:
    def __init__(self):
        self.version = "1.0.0"
        self.total_tested = 0
        self.successful = 0
        self.failed = 0
        self.lock = threading.Lock()
        
        # GitHubä»“åº“è·¯å¾„
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # ä»£ç†æ–‡ä»¶æ˜ å°„
        self.proxy_files = {
            'http': {'name': 'HTTP', 'file': 'http.txt'},
            'https': {'name': 'HTTPS', 'file': 'https.txt'}, 
            'socks4': {'name': 'SOCKS4', 'file': 'socks4.txt'},
            'socks5': {'name': 'SOCKS5', 'file': 'socks5.txt'}
        }
        
        # ç¼“å­˜æµ‹è¯•ç½‘ç«™
        self._test_urls = None
        
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        self.result_dir = os.path.join(self.base_dir, "result")
        os.makedirs(self.result_dir, exist_ok=True)
        
        print(f"ğŸ”§ğŸ”§ğŸ”§ğŸ”§ åˆå§‹åŒ–GitHubä»£ç†æµ‹è¯•å™¨")
        print(f"ğŸ“ğŸ“ğŸ“ğŸ“ å·¥ä½œç›®å½•: {self.base_dir}")
        print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ ç»“æœç›®å½•: {self.result_dir}")
    
    def clean_and_validate_proxy(self, proxy_str):
        """åˆå¹¶æ¸…ç†å’ŒéªŒè¯ä»£ç†åŠŸèƒ½ï¼Œè¿”å›æ¸…ç†åçš„ä»£ç†æˆ–Noneï¼ˆå¦‚æœæ— æ•ˆï¼‰"""
        if not proxy_str or not isinstance(proxy_str, str):
            return None
        
        proxy = proxy_str.strip()
        
        # å»æ‰/#åé¢çš„å»¶è¿Ÿä¿¡æ¯
        if '/#' in proxy:
            proxy = proxy.split('/#')[0].strip()
        
        # å¦‚æœå·²ç»æœ‰åè®®å¤´ï¼Œç›´æ¥è¿”å›
        if proxy.startswith(('http://', 'https://', 'socks4://', 'socks5://')):
            return proxy if self._validate_proxy_format(proxy) else None
        
        # ä½¿ç”¨urlparseè§£æä»£ç†æ ¼å¼
        if '://' not in proxy:
            test_url = 'http://' + proxy
        else:
            test_url = proxy
        
        try:
            parsed = urlparse(test_url)
            
            # æ£€æŸ¥ä¸»æœºå
            hostname = parsed.hostname
            if not hostname:
                return None
            
            # æ£€æŸ¥ç«¯å£
            port = parsed.port
            if port is None:
                # å°è¯•ä»netlocä¸­æå–ç«¯å£
                if ':' in parsed.netloc:
                    try:
                        port_str = parsed.netloc.split(':')[-1]
                        if '/' in port_str:
                            port_str = port_str.split('/')[0]
                        port = int(port_str)
                    except (ValueError, IndexError):
                        return None
            
            if port is not None and not (1 <= port <= 65535):
                return None
            
            # éªŒè¯IPåœ°å€æ ¼å¼ï¼ˆå¦‚æœæ˜¯IPçš„è¯ï¼‰
            if hostname.replace('.', '').isdigit():
                try:
                    socket.inet_aton(hostname)
                except socket.error:
                    # ä¸æ˜¯æœ‰æ•ˆçš„IPåœ°å€ï¼Œä½†å¯èƒ½æ˜¯åŸŸåï¼Œæ‰€ä»¥ä»ç„¶æœ‰æ•ˆ
                    pass
            
            # è¿”å›æ¸…ç†åçš„ä»£ç†æ ¼å¼
            if parsed.username or parsed.password:
                # æœ‰è®¤è¯ä¿¡æ¯çš„ä»£ç†
                auth_part = ""
                if parsed.username:
                    auth_part = parsed.username
                    if parsed.password:
                        auth_part += ":" + parsed.password
                    auth_part += "@"
                
                if hostname and port:
                    return f"{auth_part}{hostname}:{port}"
                elif hostname:
                    return hostname
            else:
                # æ²¡æœ‰è®¤è¯ä¿¡æ¯çš„ä»£ç†
                if ':' in proxy and '@' not in proxy:
                    return proxy
                elif hostname and port:
                    return f"{hostname}:{port}"
                elif hostname:
                    return hostname
            
            return proxy
            
        except Exception as e:
            # å¦‚æœurlparseè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
            if ':' in proxy:
                parts = proxy.split(':')
                if len(parts) == 2:
                    ip = parts[0].strip()
                    port_str = parts[1].strip()
                    if '/' in port_str:
                        port_str = port_str.split('/')[0]
                    
                    try:
                        port = int(port_str)
                        if 1 <= port <= 65535:
                            return f"{ip}:{port}"
                    except ValueError:
                        pass
            
            return None
    
    def _validate_proxy_format(self, proxy):
        """å†…éƒ¨éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        if not proxy:
            return False
        
        # æµ‹è¯•URLç”¨äºè§£æ
        if '://' not in proxy:
            test_url = 'http://' + proxy
        else:
            test_url = proxy
        
        try:
            parsed = urlparse(test_url)
            
            # æ£€æŸ¥ä¸»æœºå
            hostname = parsed.hostname
            if not hostname:
                return False
            
            # æ£€æŸ¥ç«¯å£
            port = parsed.port
            if port is None:
                # å°è¯•ä»netlocä¸­æå–ç«¯å£
                if ':' in parsed.netloc:
                    try:
                        port_str = parsed.netloc.split(':')[-1]
                        if '/' in port_str:
                            port_str = port_str.split('/')[0]
                        port = int(port_str)
                    except (ValueError, IndexError):
                        return False
            
            if port is not None and not (1 <= port <= 65535):
                return False
            
            # éªŒè¯IPåœ°å€æ ¼å¼ï¼ˆå¦‚æœæ˜¯IPçš„è¯ï¼‰
            if hostname.replace('.', '').isdigit():
                try:
                    socket.inet_aton(hostname)
                except socket.error:
                    # ä¸æ˜¯æœ‰æ•ˆçš„IPåœ°å€ï¼Œä½†å¯èƒ½æ˜¯åŸŸåï¼Œæ‰€ä»¥ä»ç„¶æœ‰æ•ˆ
                    pass
            
            return True
            
        except Exception:
            return False
    
    def get_proxy_url(self, proxy, proxy_type):
        """æ ¹æ®ä»£ç†ç±»å‹ç”Ÿæˆå®Œæ•´çš„ä»£ç†URL"""
        if not proxy:
            return ""
        
        # å¦‚æœä»£ç†å·²ç»æœ‰åè®®å¤´ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
        if proxy.startswith(('http://', 'https://', 'socks4://', 'socks5://')):
            current_protocol = proxy.split('://')[0] + '://'
            target_protocol = ""
            
            if proxy_type == "HTTP":
                target_protocol = "http://"
            elif proxy_type == "HTTPS":
                target_protocol = "https://"
            elif proxy_type == "SOCKS4":
                target_protocol = "socks4://"
            elif proxy_type == "SOCKS5":
                target_protocol = "socks5://"
            else:
                target_protocol = "http://"
            
            # å¦‚æœå½“å‰åè®®ä¸ç›®æ ‡åè®®ä¸åŒï¼Œè¿›è¡Œè½¬æ¢
            if current_protocol != target_protocol:
                return proxy.replace(current_protocol, target_protocol, 1)
            else:
                return proxy
        
        # ä»£ç†æ²¡æœ‰åè®®å¤´ï¼Œæ·»åŠ å¯¹åº”çš„åè®®
        if proxy_type == "HTTP":
            return f"http://{proxy}"
        elif proxy_type == "HTTPS":
            return f"https://{proxy}"
        elif proxy_type == "SOCKS4":
            return f"socks4://{proxy}"
        elif proxy_type == "SOCKS5":
            return f"socks5://{proxy}"
        else:
            return f"http://{proxy}"
    
    def extract_domain_info(self, url):
        """ä»URLä¸­æå–åŸŸåä¿¡æ¯å’Œcheck_string"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            if not domain:
                domain = url.split('/')[2] if '//' in url else url.split('/')[0]
            
            domain = domain.split(':')[0]
            parts = domain.split('.')
            
            if len(parts) >= 2:
                check_string = parts[-2]
                site_abbr = check_string[:3].lower()
                return check_string, site_abbr
            else:
                check_string = domain
                site_abbr = domain[:3].lower() if len(domain) >= 3 else domain.lower()
                return check_string, site_abbr
                
        except Exception as e:
            print(f"âŒâŒâŒâŒ è§£æURL {url} å¤±è´¥: {e}")
            return "website", "web"
    
    def load_test_urls(self):
        """åŠ è½½æµ‹è¯•ç½‘ç«™åˆ—è¡¨"""
        if self._test_urls is not None:
            return self._test_urls
        
        ym_file = os.path.join(self.base_dir, "ym.txt")
        
        if not os.path.exists(ym_file):
            print(f"âŒâŒâŒâŒ ym.txtæ–‡ä»¶ä¸å­˜åœ¨: {ym_file}")
            return []
        
        test_urls = []
        
        try:
            with open(ym_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if not line.startswith(('http://', 'https://')):
                            line = 'https://' + line
                        
                        check_string, site_abbr = self.extract_domain_info(line)
                        
                        test_urls.append({
                            "url": line,
                            "name": f"ç½‘ç«™{line_num}",
                            "timeout": 8,
                            "check_string": check_string,
                            "site_abbr": site_abbr
                        })
            
            # å»é‡
            unique_test_urls = []
            seen_urls = set()
            for site in test_urls:
                url_key = site["url"].rstrip('/')
                if url_key not in seen_urls:
                    seen_urls.add(url_key)
                    unique_test_urls.append(site)
            
            self._test_urls = unique_test_urls
            
            if unique_test_urls:
                print(f"âœ… ä»ym.txtåŠ è½½äº† {len(unique_test_urls)} ä¸ªæµ‹è¯•ç½‘ç«™")
            else:
                print("âŒâŒâŒâŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç½‘ç«™")
            
            return self._test_urls
            
        except Exception as e:
            print(f"âŒâŒâŒâŒ è¯»å–ym.txtå¤±è´¥: {e}")
            return []
    
    def get_test_urls(self):
        """è·å–æµ‹è¯•ç½‘ç«™åˆ—è¡¨"""
        if self._test_urls is None:
            return self.load_test_urls()
        return self._test_urls
    
    def import_previous_successful_proxies(self):
        """å¯¼å…¥ä¸Šæ¬¡æµ‹è¯•æˆåŠŸçš„ä»£ç†åˆ°å¯¹åº”æ–‡ä»¶"""
        if not os.path.exists(self.result_dir):
            print("âŒâŒâŒâŒ resultç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¼å…¥")
            return
        
        imported_count = 0
        
        for proxy_type, info in self.proxy_files.items():
            result_file = os.path.join(self.result_dir, f"{proxy_type}.txt")
            
            if os.path.exists(result_file):
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        successful_proxies = []
                        
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                # ä½¿ç”¨åˆå¹¶çš„æ¸…ç†å’ŒéªŒè¯åŠŸèƒ½
                                proxy = self.clean_and_validate_proxy(line)
                                if proxy:
                                    successful_proxies.append(proxy)
                    
                    if successful_proxies:
                        target_path = os.path.join(self.base_dir, info['file'])
                        existing_proxies = []
                        
                        if os.path.exists(target_path):
                            with open(target_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    line = line.strip()
                                    if line and not line.startswith('#'):
                                        # ä½¿ç”¨åˆå¹¶çš„æ¸…ç†å’ŒéªŒè¯åŠŸèƒ½
                                        proxy = self.clean_and_validate_proxy(line)
                                        if proxy:
                                            existing_proxies.append(proxy)
                        
                        all_proxies = list(set(existing_proxies + successful_proxies))
                        
                        with open(target_path, 'w', encoding='utf-8') as f:
                            f.write(f"# {info['name']}ä»£ç†åˆ—è¡¨ï¼ˆåŒ…å«å¯¼å…¥çš„æˆåŠŸä»£ç†ï¼‰\n")
                            f.write(f"# æ€»ä»£ç†æ•°: {len(all_proxies)}\n")
                            f.write(f"# å¯¼å…¥æˆåŠŸä»£ç†: {len(successful_proxies)}\n")
                            f.write("#" * 50 + "\n\n")
                            
                            for proxy in sorted(all_proxies):
                                f.write(f"{proxy}\n")
                        
                        print(f"âœ… å¯¼å…¥ {len(successful_proxies)} ä¸ªæˆåŠŸ{info['name']}ä»£ç†")
                        imported_count += len(successful_proxies)
                        
                except Exception as e:
                    print(f"âŒâŒâŒâŒ å¯¼å…¥{info['name']}ä»£ç†å¤±è´¥: {e}")
        
        if imported_count > 0:
            print(f"âœ… æ€»å…±å¯¼å…¥ {imported_count} ä¸ªæˆåŠŸä»£ç†")
        else:
            print("â„¹â„¹â„¹â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å…¥çš„æˆåŠŸä»£ç†")
    
    def download_proxy_list(self, url, proxy_type):
        """ä»æŒ‡å®šURLä¸‹è½½ä»£ç†åˆ—è¡¨"""
        print(f"ğŸŒğŸŒğŸŒğŸŒ ä¸‹è½½{proxy_type}ä»£ç†: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            }
            
            # ä¸‹è½½ä»£ç†åˆ—è¡¨æ—¶ä¸ä½¿ç”¨ä»£ç†ï¼ŒéªŒè¯SSLè¯ä¹¦
            response = requests.get(url, headers=headers, timeout=30, verify=True)
            
            if response.status_code == 200:
                proxies = []
                for line in response.text.splitlines():
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('//'):
                        # ä½¿ç”¨åˆå¹¶çš„æ¸…ç†å’ŒéªŒè¯åŠŸèƒ½
                        proxy = self.clean_and_validate_proxy(line)
                        if proxy:
                            proxies.append(proxy)
                
                print(f"âœ… ä¸‹è½½åˆ° {len(proxies)} ä¸ª{proxy_type}ä»£ç†")
                return proxies
            else:
                print(f"âŒâŒâŒâŒ ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒâŒâŒâŒ ä¸‹è½½å¤±è´¥: {e}")
            return []
    
    def save_proxies_to_file(self, proxies, file_path, proxy_type):
        """ä¿å­˜ä»£ç†åˆ—è¡¨åˆ°æ–‡ä»¶"""
        if not proxies:
            print(f"âš ï¸ æ²¡æœ‰{proxy_type}ä»£ç†å¯ä¿å­˜")
            return
        
        unique_proxies = list(set(proxies))
        print(f"ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š {proxy_type}ä»£ç†å»é‡å: {len(unique_proxies)} ä¸ª")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {proxy_type}ä»£ç†åˆ—è¡¨\n")
            f.write(f"# æ€»ä»£ç†æ•°: {len(unique_proxies)}\n")
            f.write("#" * 50 + "\n\n")
            
            for proxy in sorted(unique_proxies):
                f.write(f"{proxy}\n")
        
        print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ å·²ä¿å­˜åˆ°: {file_path}")
    
    def parse_source_file(self):
        """è§£æsource.txtæ–‡ä»¶ï¼Œæå–ä¸‹è½½é“¾æ¥"""
        source_file = os.path.join(self.base_dir, "source.txt")
        
        if not os.path.exists(source_file):
            print(f"âŒâŒâŒâŒ source.txtæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
            return {}
        
        print(f"ğŸ“ğŸ“ğŸ“ğŸ“ è§£æsource.txtæ–‡ä»¶")
        
        proxy_type_mapping = {
            'http': 'http',
            'https': 'https', 
            'socks4': 'socks4',
            'socks5': 'socks5'
        }
        
        all_links = {
            'http': [],
            'https': [],
            'socks4': [],
            'socks5': []
        }
        
        try:
            with open(source_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
                
                try:
                    data = json.loads(content)
                    
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict):
                                for proxy_type, urls in item.items():
                                    if proxy_type.lower() in proxy_type_mapping:
                                        mapped_type = proxy_type_mapping[proxy_type.lower()]
                                        
                                        if isinstance(urls, str):
                                            if urls.startswith('[') and urls.endswith(']'):
                                                try:
                                                    url_list = json.loads(urls)
                                                    if isinstance(url_list, list):
                                                        for url in url_list:
                                                            all_links[mapped_type].append(url)
                                                except:
                                                    all_links[mapped_type].append(urls)
                                            else:
                                                all_links[mapped_type].append(urls)
                                        elif isinstance(urls, list):
                                            for url in urls:
                                                all_links[mapped_type].append(url)
                    
                    print(f"âœ… è§£ææˆåŠŸï¼Œæ‰¾åˆ° {sum(len(links) for links in all_links.values())} ä¸ªé“¾æ¥")
                    
                except json.JSONDecodeError as e:
                    print(f"âŒâŒâŒâŒ JSONè§£æå¤±è´¥: {e}")
                    return {}
            
            total_links = sum(len(links) for links in all_links.values())
            print(f"ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š æ€»å…±æ‰¾åˆ° {total_links} ä¸ªä¸‹è½½é“¾æ¥")
            
            return all_links
            
        except Exception as e:
            print(f"âŒâŒâŒâŒ è§£æsource.txtæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def download_and_classify_proxies(self):
        """ä»source.txtä¸‹è½½ä»£ç†å¹¶åˆ†ç±»ä¿å­˜"""
        print("\n" + "="*60)
        print("ğŸ“¥ğŸ“¥ğŸ“¥ğŸ“¥ å¼€å§‹ä¸‹è½½ä»£ç†å¹¶åˆ†ç±»")
        print("="*60)
        
        all_links = self.parse_source_file()
        
        if not any(all_links.values()):
            print("âŒâŒâŒâŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥")
            return
        
        total_downloaded = 0
        
        for proxy_type, links in all_links.items():
            if not links:
                continue
                
            print(f"\nğŸ“¥ğŸ“¥ğŸ“¥ğŸ“¥ å¤„ç†{proxy_type.upper()}ä»£ç†...")
            
            all_proxies = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {
                    executor.submit(self.download_proxy_list, url, proxy_type): url 
                    for url in links
                }
                
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        proxies = future.result()
                        all_proxies.extend(proxies)
                    except Exception as e:
                        print(f"âŒâŒâŒâŒ ä¸‹è½½å¤±è´¥: {e}")
            
            if all_proxies:
                file_path = os.path.join(self.base_dir, self.proxy_files[proxy_type]['file'])
                self.save_proxies_to_file(all_proxies, file_path, proxy_type.upper())
                total_downloaded += len(all_proxies)
            else:
                print(f"âŒâŒâŒâŒ æ²¡æœ‰ä¸‹è½½åˆ°æœ‰æ•ˆçš„{proxy_type}ä»£ç†")
        
        print(f"\nğŸ“¥ğŸ“¥ğŸ“¥ğŸ“¥ ä¸‹è½½å®Œæˆï¼Œå¼€å§‹å¯¼å…¥ä¸Šæ¬¡æµ‹è¯•æˆåŠŸçš„ä»£ç†...")
        self.import_previous_successful_proxies()
        
        print(f"\nâœ… ä¸‹è½½å’Œå¯¼å…¥å®Œæˆ! æ€»å…±ä¸‹è½½ {total_downloaded} ä¸ªä»£ç†")
        return total_downloaded
    
    def load_proxies(self, file_path, limit=0):
        """ä»æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨"""
        print(f"ğŸ“ğŸ“ğŸ“ğŸ“ åŠ è½½ä»£ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        
        if not os.path.exists(file_path):
            print(f"âŒâŒâŒâŒ æ–‡ä»¶ä¸å­˜åœ¨!")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                proxies = []
                lines = 0
                
                for line in f:
                    line = line.strip()
                    
                    if not line or line.startswith('#') or line.startswith('//'):
                        continue
                    
                    # ä½¿ç”¨åˆå¹¶çš„æ¸…ç†å’ŒéªŒè¯åŠŸèƒ½
                    proxy = self.clean_and_validate_proxy(line)
                    if proxy:
                        proxies.append(proxy)
                        lines += 1
                    
                    if limit > 0 and lines >= limit:
                        break
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(proxies)} ä¸ªä»£ç†")
                if limit > 0 and lines >= limit:
                    print(f"ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š åªåŠ è½½å‰ {limit} ä¸ªä»£ç†")
                
                return proxies
                
        except Exception as e:
            print(f"âŒâŒâŒâŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def test_single_url(self, proxy, test_config, proxy_type):
        """æµ‹è¯•å•ä¸ªURL - åªåœ¨requests.getä¸­ä½¿ç”¨verify=False"""
        result = {
            'proxy': proxy,
            'proxy_type': proxy_type,
            'test_name': test_config['name'],
            'test_url': test_config['url'],
            'success': False,
            'latency_ms': 0,
            'status_code': 0,
            'error': None,
            'timestamp': datetime.now().strftime("%H:%M:%S"),
            'site_abbr': test_config.get('site_abbr', 'web')
        }
        
        # ç”Ÿæˆä»£ç†URL
        proxy_url = self.get_proxy_url(proxy, proxy_type)
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        
        try:
            start_time = time.time()
            
            # åªåœ¨æµ‹è¯•ä»£ç†æ—¶ç¦ç”¨SSLéªŒè¯
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = requests.get(
                    test_config['url'],
                    proxies=proxies,
                    timeout=test_config.get('timeout', 8),
                    headers=headers,
                    verify=False,  # åªåœ¨æµ‹è¯•ä»£ç†æ—¶ç¦ç”¨SSLéªŒè¯
                    allow_redirects=True
                )
            
            latency = time.time() - start_time
            result['latency_ms'] = latency * 1000
            result['status_code'] = response.status_code
            
            if response.status_code == 200:
                check_string = test_config.get('check_string', '')
                if check_string:
                    if check_string.lower() in response.text.lower():
                        result['success'] = True
                        result['error'] = None
                    else:
                        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«å¸¸è§HTMLæ ‡è®°
                        page_text = response.text.lower()
                        common_indicators = ['html', 'http', 'www', 'com', 'net', 'org', 'title', 'body', 'head']
                        indicators_found = sum(1 for indicator in common_indicators if indicator in page_text)
                        
                        if indicators_found >= 2:  # æé«˜é˜ˆå€¼å‡å°‘è¯¯åˆ¤
                            result['success'] = True
                            result['error'] = f"æœªæ‰¾åˆ° '{check_string}' ä½†é¡µé¢æœ‰æ•ˆ"
                        else:
                            result['error'] = f"æœªæ‰¾åˆ° '{check_string}' ä¸”é¡µé¢æ— æ•ˆ"
                else:
                    result['success'] = True
                    result['error'] = None
            else:
                result['error'] = f"HTTP {response.status_code}"
                
        except requests.exceptions.ConnectTimeout:
            result['error'] = 'è¿æ¥è¶…æ—¶'
        except requests.exceptions.ReadTimeout:
            result['error'] = 'è¯»å–è¶…æ—¶'
        except requests.exceptions.ConnectionError as e:
            error_str = str(e)
            if 'timed out' in error_str.lower():
                result['error'] = 'è¿æ¥è¶…æ—¶'
            elif 'reset' in error_str.lower():
                result['error'] = 'è¿æ¥è¢«é‡ç½®'
            elif 'refused' in error_str.lower():
                result['error'] = 'è¿æ¥è¢«æ‹’ç»'
            else:
                result['error'] = f'è¿æ¥é”™è¯¯: {error_str[:30]}'
        except requests.exceptions.ProxyError as e:
            error_str = str(e)
            if 'timed out' in error_str.lower():
                result['error'] = 'ä»£ç†è¶…æ—¶'
            elif 'socks' in error_str.lower():
                result['error'] = 'SOCKSä»£ç†é”™è¯¯'
            else:
                result['error'] = f'ä»£ç†é”™è¯¯: {error_str[:30]}'
        except requests.exceptions.SSLError as e:
            result['error'] = f'SSLé”™è¯¯: {str(e)[:30]}'
        except socket.timeout:
            result['error'] = 'Socketè¶…æ—¶'
        except Exception as e:
            error_str = str(e)
            result['error'] = f'å…¶ä»–é”™è¯¯: {error_str[:30]}'
        
        return result
    
    def test_proxy_connectivity(self, proxy, proxy_type):
        """æµ‹è¯•å•ä¸ªä»£ç†çš„è¿é€šæ€§"""
        test_urls = self.get_test_urls()
        best_result = None
        
        for test_config in test_urls:
            result = self.test_single_url(proxy, test_config, proxy_type)
            
            if result['success']:
                return result
            
            if best_result is None or (result.get('status_code', 0) > 0 and best_result.get('status_code', 0) == 0):
                best_result = result
            
            if result.get('status_code', 0) > 0:
                break
        
        return best_result or {
            'proxy': proxy,
            'proxy_type': proxy_type,
            'success': False,
            'error': 'æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥',
            'latency_ms': 0,
            'test_name': 'ç»¼åˆæµ‹è¯•',
            'test_url': 'å¤šä¸ªURL',
            'timestamp': datetime.now().strftime("%H:%M:%S"),
            'site_abbr': 'unk'
        }
    
    def batch_test_proxies(self, proxies, proxy_type, max_workers=20):
        """æ‰¹é‡æµ‹è¯•ä»£ç†"""
        if not proxies:
            return [], []
        
        print(f"\nğŸš€ğŸš€ğŸš€ğŸš€ å¼€å§‹æµ‹è¯• {len(proxies)} ä¸ª{proxy_type}ä»£ç†")
        print(f"ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š å¹¶å‘çº¿ç¨‹: {max_workers}")
        print(f"â±â±â±â±â±â±â±â±â± è¶…æ—¶æ—¶é—´: 8ç§’")
        print("-"*50)
        
        all_results = []
        successful_results = []
        
        def worker(proxy):
            result = self.test_proxy_connectivity(proxy, proxy_type)
            
            with self.lock:
                all_results.append(result)
                
                if result['success']:
                    successful_results.append(result)
                
                self.total_tested += 1
                if result['success']:
                    self.successful += 1
                else:
                    self.failed += 1
                
                if self.total_tested % 10 == 0 or self.total_tested == len(proxies):
                    percentage = self.total_tested / len(proxies) * 100
                    print(f"\rğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ è¿›åº¦: {self.total_tested}/{len(proxies)} "
                          f"[{percentage:.1f}%] | "
                          f"âœ…: {self.successful} | "
                          f"âŒâŒâŒâŒ: {self.failed}", end="")
            
            return result
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(worker, proxy): proxy for proxy in proxies}
            for future in concurrent.futures.as_completed(futures):
                pass
        
        total_time = time.time() - start_time
        
        print()
        print(f"â±â±â±â±â±â±â±â±â± æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š å¹³å‡é€Ÿåº¦: {len(proxies)/total_time:.1f}ä¸ª/ç§’")
        
        return all_results, successful_results
    
    def display_results(self, all_results, successful_results, proxy_type):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        
        total = len(all_results)
        success_rate = (self.successful / total * 100) if total > 0 else 0
        
        print(f"ä»£ç†ç±»å‹: {proxy_type}")
        print(f"æ€»ä»£ç†æ•°: {total}")
        print(f"æˆåŠŸä»£ç†: {self.successful} ({success_rate:.1f}%)")
        print(f"å¤±è´¥ä»£ç†: {total - self.successful}")
        
        if successful_results:
            site_stats = {}
            for result in successful_results:
                site_abbr = result.get('site_abbr', 'unk')
                site_stats[site_abbr] = site_stats.get(site_abbr, 0) + 1
            
            print(f"\nğŸŒğŸŒğŸŒğŸŒ æˆåŠŸç½‘ç«™åˆ†å¸ƒ:")
            for site_abbr, count in site_stats.items():
                print(f"  {site_abbr.upper()}æˆåŠŸ: {count}ä¸ª")
        
        if successful_results:
            latencies = [r['latency_ms'] for r in successful_results]
            avg_latency = sum(latencies) / len(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            
            print(f"\nâ±â±â±â±â±â±â±â±â± å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms")
            print(f"  æœ€å¿«å»¶è¿Ÿ: {min_latency:.0f}ms")
            print(f"  æœ€æ…¢å»¶è¿Ÿ: {max_latency:.0f}ms")
            
            print(f"\nğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒ:")
            latency_ranges = [
                (0, 100, "æå¿« <100ms"),
                (100, 200, "å¿«é€Ÿ 100-200ms"),
                (200, 500, "ä¸­ç­‰ 200-500ms"),
                (500, 1000, "è¾ƒæ…¢ 500ms-1s"),
                (1000, 3000, "æ…¢ 1-3s"),
                (3000, float('inf'), "å¾ˆæ…¢ >3s")
            ]
            
            for min_r, max_r, label in latency_ranges:
                count = sum(1 for r in successful_results if min_r <= r['latency_ms'] < max_r)
                if count > 0:
                    percentage = count / len(successful_results) * 100
                    bar_length = int(percentage / 5)
                    bar = "â–ˆ" * bar_length
                    print(f"  {label:15s}: {count:3d}ä¸ª ({percentage:5.1f}%) {bar}")
        
        if successful_results:
            fastest = sorted(successful_results, key=lambda x: x['latency_ms'])[:5]
            
            print(f"\nğŸš€ğŸš€ğŸš€ğŸš€ æœ€å¿«çš„5ä¸ªä»£ç†:")
            for i, result in enumerate(fastest, 1):
                latency = result['latency_ms']
                site_abbr = result.get('site_abbr', 'unk')
                
                if latency < 100:
                    indicator = "ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢"
                elif latency < 200:
                    indicator = "ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡ğŸŸ¡"
                elif latency < 500:
                    indicator = "ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ¡ğŸŸ¡ğŸŸ¡"
                elif latency < 1000:
                    indicator = "ğŸ”´ğŸ”´ğŸ”´ğŸ”´"
                else:
                    indicator = "âš«âš«âš«âš«"
                
                print(f"  {i}. {indicator} {result['proxy']:20s} | {latency:5.0f}ms | {site_abbr}")
        
        failed_results = [r for r in all_results if not r['success']]
        if failed_results:
            error_stats = {}
            for r in failed_results:
                error = r.get('error', 'æœªçŸ¥é”™è¯¯')
                error_key = error.split(':')[0] if ':' in error else error
                error_stats[error_key] = error_stats.get(error_key, 0) + 1
            
            print(f"\nâŒâŒâŒâŒ å¤±è´¥åŸå› åˆ†æ ({len(failed_results)} ä¸ª):")
            for error, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = count / len(failed_results) * 100
                print(f"  {error:30s}: {count:3d}ä¸ª ({percentage:5.1f}%)")
    
    def extract_proxy_info_from_txt(self, txt_file_path, proxy_type):
        """ä»txtæ–‡ä»¶ä¸­æå–ä»£ç†ä¿¡æ¯ï¼Œå»æ‰åè®®å¤´"""
        proxies_list = []
        
        try:
            with open(txt_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # æå–/#ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
                        if '/#' in line:
                            full_proxy = line.split('/#')[0].strip()
                        else:
                            full_proxy = line
                        
                        # ä½¿ç”¨åˆå¹¶çš„æ¸…ç†å’ŒéªŒè¯åŠŸèƒ½
                        proxy = self.clean_and_validate_proxy(full_proxy)
                        if proxy:
                            proxies_list.append(proxy)
            
            print(f"âœ… ä»txtæ–‡ä»¶ä¸­æå–äº† {len(proxies_list)} ä¸ª{proxy_type}ä»£ç†")
            return proxies_list
            
        except Exception as e:
            print(f"âŒâŒâŒâŒ ä»txtæ–‡ä»¶æå–ä»£ç†ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def save_results(self, all_results, successful_results, proxy_type):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°resultæ–‡ä»¶å¤¹"""
        if successful_results:
            successful_sorted = sorted(successful_results, key=lambda x: x['latency_ms'])
            
            # 1. ä¿å­˜ä¸ºtxtæ ¼å¼ï¼ˆåŸæœ‰æ ¼å¼ï¼‰
            result_txt_file = os.path.join(self.result_dir, f"{proxy_type.lower()}.txt")
            
            with open(result_txt_file, 'w', encoding='utf-8') as f:
                f.write(f"# {proxy_type}ä»£ç†æµ‹è¯•ç»“æœ - æœ‰æ•ˆä»£ç†åˆ—è¡¨\n")
                f.write(f"# æ€»æµ‹è¯•æ•°: {len(all_results)}\n")
                f.write(f"# æˆåŠŸä»£ç†: {len(successful_results)}\n")
                f.write(f"# æˆåŠŸç‡: {len(successful_results)/len(all_results)*100:.1f}%\n")
                f.write(f"# æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ ¼å¼: åè®®://IP:ç«¯å£/#å»¶è¿Ÿms%20ç½‘ç«™ç¼©å†™\n")
                f.write("#"*60 + "\n\n")
                
                for i, result in enumerate(successful_sorted, 1):
                    proxy = result['proxy']
                    latency = result['latency_ms']
                    site_abbr = result.get('site_abbr', 'unk')
                    
                    if latency.is_integer():
                        latency_str = f"{int(latency)}ms"
                    else:
                        latency_str = f"{latency:.1f}ms"
                    
                    proxy_url = self.get_proxy_url(proxy, proxy_type)
                    f.write(f"{proxy_url}/#{latency_str}%20{site_abbr}\n")
            
            print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ TXTç»“æœå·²ä¿å­˜: {result_txt_file}")
            
            # 2. ä»txtæ–‡ä»¶ä¸­æå–ä»£ç†ä¿¡æ¯å¹¶ä¿å­˜ä¸ºJSONæ ¼å¼
            json_proxies = self.extract_proxy_info_from_txt(result_txt_file, proxy_type)
            
            if json_proxies:
                result_json_file = os.path.join(self.result_dir, f"{proxy_type.lower()}.json")
                json_data = {"ts": json_proxies}
                
                with open(result_json_file, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ JSONç»“æœå·²ä¿å­˜: {result_json_file}")
                print(f"ğŸ“‹ğŸ“‹ğŸ“‹ğŸ“‹ JSONæ ¼å¼: {{'ts': [ä»£ç†åˆ—è¡¨]}}")
            
            return result_txt_file
        else:
            print(f"âŒâŒâŒâŒ æ²¡æœ‰æœ‰æ•ˆçš„{proxy_type}ä»£ç†ï¼Œæœªä¿å­˜ç»“æœ")
            return None
    
    def test_proxy_type(self, proxy_type, max_workers=20, limit=0):
        """æµ‹è¯•ç‰¹å®šç±»å‹çš„ä»£ç†"""
        if proxy_type not in self.proxy_files:
            print(f"âŒâŒâŒâŒ æ— æ•ˆçš„ä»£ç†ç±»å‹: {proxy_type}")
            return
        
        info = self.proxy_files[proxy_type]
        file_path = os.path.join(self.base_dir, info['file'])
        
        if not os.path.exists(file_path):
            print(f"âŒâŒâŒâŒ ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        print(f"\n" + "="*60)
        print(f"ğŸ¯ğŸ¯ğŸ¯ğŸ¯ å¼€å§‹æµ‹è¯• {info['name']} ä»£ç†")
        print(f"ğŸ“ğŸ“ğŸ“ğŸ“ ä»£ç†æ–‡ä»¶: {file_path}")
        print("="*60)
        
        # é‡ç½®è®¡æ•°å™¨
        self.total_tested = 0
        self.successful = 0
        self.failed = 0
        
        # åŠ è½½ä»£ç†
        proxies = self.load_proxies(file_path, limit)
        
        if not proxies:
            print("âŒâŒâŒâŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»£ç†ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ä»£ç†
        all_results, successful_results = self.batch_test_proxies(
            proxies=proxies,
            proxy_type=info['name'],
            max_workers=max_workers
        )
        
        # æ˜¾ç¤ºç»“æœ
        self.display_results(all_results, successful_results, info['name'])
        
        # ä¿å­˜ç»“æœ
        saved_file = self.save_results(all_results, successful_results, info['name'])
        
        if successful_results:
            fastest = min(successful_results, key=lambda x: x['latency_ms'])
            
            print(f"\nğŸ¯ğŸ¯ğŸ¯ğŸ¯ ä½¿ç”¨ç¤ºä¾‹:")
            proxy_url = self.get_proxy_url(fastest['proxy'], info['name'])
            print(f"  # åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨:")
            print(f"  curl -x {proxy_url} {fastest.get('test_url', 'https://example.com')}")
            
            print(f"\n  # åœ¨Pythonä¸­ä½¿ç”¨:")
            print(f"  import requests")
            print(f"  proxies = {{'http': '{proxy_url}', 'https': '{proxy_url}'}}")
            print(f"  response = requests.get('{fastest.get('test_url', 'https://example.com')}', proxies=proxies)")
        
        print(f"\n{'='*60}")
        print(f"âœ… {info['name']}ä»£ç†æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“…ğŸ“…ğŸ“…ğŸ“… å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        return len(successful_results)
    
    def auto_run(self):
        """è‡ªåŠ¨è¿è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
        print("ğŸš€ğŸš€ğŸš€ğŸš€ å¼€å§‹GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•")
        print(f"ğŸ“ğŸ“ğŸ“ğŸ“ å·¥ä½œç›®å½•: {self.base_dir}")
        print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ ç»“æœç›®å½•: {self.result_dir}")
        print("="*60)
        
        start_time = time.time()
        
        # 1. ä¸‹è½½ä»£ç†
        print("\nğŸ“¥ğŸ“¥ğŸ“¥ğŸ“¥ æ­¥éª¤1: ä¸‹è½½ä»£ç†")
        downloaded_count = self.download_and_classify_proxies()
        
        # 2. å¯¼å…¥ä¸Šæ¬¡æˆåŠŸçš„ä»£ç†
        print("\nğŸ“¥ğŸ“¥ğŸ“¥ğŸ“¥ æ­¥éª¤2: å¯¼å…¥ä¸Šæ¬¡æˆåŠŸçš„ä»£ç†")
        self.import_previous_successful_proxies()
        
        # 3. ä¾æ¬¡æµ‹è¯•å„ç§ç±»å‹çš„ä»£ç†
        print("\nğŸ§ªğŸ§ªğŸ§ªğŸ§ª æ­¥éª¤3: å¼€å§‹æµ‹è¯•ä»£ç†")
        
        test_results = {}
        proxy_types = ['http', 'https', 'socks4', 'socks5']
        
        for proxy_type in proxy_types:
            if proxy_type in self.proxy_files:
                info = self.proxy_files[proxy_type]
                file_path = os.path.join(self.base_dir, info['file'])
                
                if os.path.exists(file_path):
                    print(f"\n" + "="*60)
                    print(f"ğŸ§ªğŸ§ªğŸ§ªğŸ§ª æµ‹è¯• {info['name']} ä»£ç†")
                    print("="*60)
                    
                    successful_count = self.test_proxy_type(proxy_type, max_workers=20, limit=0)
                    test_results[proxy_type] = successful_count
                    
                    # çŸ­æš‚æš‚åœï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
                    time.sleep(2)
                else:
                    print(f"âŒâŒâŒâŒ è·³è¿‡{info['name']}ä»£ç†æµ‹è¯•ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    test_results[proxy_type] = 0
            else:
                print(f"âŒâŒâŒâŒ æœªçŸ¥ä»£ç†ç±»å‹: {proxy_type}")
                test_results[proxy_type] = 0
        
        # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        total_successful = sum(test_results.values())
        print(f"ğŸ“ˆğŸ“ˆğŸ“ˆğŸ“ˆ æ€»æˆåŠŸä»£ç†æ•°: {total_successful}")
        
        for proxy_type, count in test_results.items():
            if proxy_type in self.proxy_files:
                info = self.proxy_files[proxy_type]
                print(f"  {info['name']}: {count} ä¸ª")
        
        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        
        print(f"\nâ±â±â±â±â±â±â±â±â± æ€»è€—æ—¶: {minutes}åˆ†{seconds}ç§’")
        print(f"ğŸ’¾ğŸ’¾ğŸ’¾ğŸ’¾ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {self.result_dir}")
        
        print("\nâœ… GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    tester = GitHubProxyTester()
    
    try:
        # è‡ªåŠ¨è¿è¡Œå®Œæ•´æµç¨‹
        tester.auto_run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒâŒâŒâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
