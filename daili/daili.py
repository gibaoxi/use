#!/usr/bin/env python3
"""
GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•è„šæœ¬ - æ”¯æŒHTTPã€HTTPSã€SOCKS4ã€SOCKS5ä»£ç†æµ‹è¯•
"""

import os
import sys
import time
import json
import socket
import requests
import concurrent.futures
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import urllib3
import threading
import re
from urllib.parse import urlparse

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class GitHubProxyTester:
    def __init__(self):
        self.version = "1.0.0"
        self.total_tested = 0
        self.successful = 0
        self.failed = 0
        self.lock = threading.Lock()
        
        # GitHubä»“åº“è·¯å¾„
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # ä»£ç†æ–‡ä»¶æ˜ å°„
        self.proxy_files = {
            'http': {'name': 'HTTP', 'file': 'http.txt'},
            'https': {'name': 'HTTPS', 'file': 'https.txt'}, 
            'socks4': {'name': 'SOCKS4', 'file': 'sock4.txt'},
            'socks5': {'name': 'SOCKS5', 'file': 'sock5.txt'}
        }
        
        # ç¼“å­˜æµ‹è¯•ç½‘ç«™
        self._test_urls = None
        
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        self.result_dir = os.path.join(self.base_dir, "result")
        os.makedirs(self.result_dir, exist_ok=True)
        
        print(f"ğŸ”§ğŸ”§ åˆå§‹åŒ–GitHubä»£ç†æµ‹è¯•å™¨")
        print(f"ğŸ“ğŸ“ å·¥ä½œç›®å½•: {self.base_dir}")
        print(f"ğŸ’¾ğŸ’¾ ç»“æœç›®å½•: {self.result_dir}")
    
    def extract_domain_info(self, url):
        """ä»URLä¸­æå–åŸŸåä¿¡æ¯å’Œcheck_string"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            if not domain:
                domain = url.split('/')[2] if '//' in url else url.split('/')[0]
            
            domain = domain.split(':')[0]
            parts = domain.split('.')
            
            if len(parts) >= 2:
                check_string = parts[-2]
                site_abbr = check_string[:3].lower()
                return check_string, site_abbr
            else:
                check_string = domain
                site_abbr = domain[:3].lower() if len(domain) >= 3 else domain.lower()
                return check_string, site_abbr
                
        except Exception as e:
            print(f"âŒâŒ è§£æURL {url} å¤±è´¥: {e}")
            return "website", "web"
    
    def load_test_urls(self):
        """åŠ è½½æµ‹è¯•ç½‘ç«™åˆ—è¡¨"""
        if self._test_urls is not None:
            return self._test_urls
        
        ym_file = os.path.join(self.base_dir, "ym.txt")
        
        if not os.path.exists(ym_file):
            print(f"âŒâŒ ym.txtæ–‡ä»¶ä¸å­˜åœ¨: {ym_file}")
            return []
        
        test_urls = []
        
        try:
            with open(ym_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if not line.startswith(('http://', 'https://')):
                            line = 'https://' + line
                        
                        check_string, site_abbr = self.extract_domain_info(line)
                        
                        test_urls.append({
                            "url": line,
                            "name": f"ç½‘ç«™{line_num}",
                            "timeout": 8,
                            "check_string": check_string,
                            "site_abbr": site_abbr
                        })
            
            # å»é‡
            unique_test_urls = []
            seen_urls = set()
            for site in test_urls:
                url_key = site["url"].rstrip('/')
                if url_key not in seen_urls:
                    seen_urls.add(url_key)
                    unique_test_urls.append(site)
            
            self._test_urls = unique_test_urls
            
            if unique_test_urls:
                print(f"âœ… ä»ym.txtåŠ è½½äº† {len(unique_test_urls)} ä¸ªæµ‹è¯•ç½‘ç«™")
            else:
                print("âŒâŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç½‘ç«™")
            
            return self._test_urls
            
        except Exception as e:
            print(f"âŒâŒ è¯»å–ym.txtå¤±è´¥: {e}")
            return []
    
    def get_test_urls(self):
        """è·å–æµ‹è¯•ç½‘ç«™åˆ—è¡¨"""
        if self._test_urls is None:
            return self.load_test_urls()
        return self._test_urls
    
    def import_previous_successful_proxies(self):
        """å¯¼å…¥ä¸Šæ¬¡æµ‹è¯•æˆåŠŸçš„ä»£ç†åˆ°å¯¹åº”æ–‡ä»¶"""
        if not os.path.exists(self.result_dir):
            print("âŒâŒ resultç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯¼å…¥")
            return
        
        imported_count = 0
        
        for proxy_type, info in self.proxy_files.items():
            result_file = os.path.join(self.result_dir, f"{proxy_type}.txt")
            
            if os.path.exists(result_file):
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        successful_proxies = []
                        
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                proxy = line.split('/#')[0].strip()
                                if proxy and self.validate_proxy(proxy):
                                    successful_proxies.append(proxy)
                    
                    if successful_proxies:
                        target_path = os.path.join(self.base_dir, info['file'])
                        existing_proxies = []
                        
                        if os.path.exists(target_path):
                            with open(target_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    line = line.strip()
                                    if line and not line.startswith('#'):
                                        proxy = self.clean_proxy(line)
                                        if proxy and self.validate_proxy(proxy):
                                            existing_proxies.append(proxy)
                        
                        all_proxies = list(set(existing_proxies + successful_proxies))
                        
                        with open(target_path, 'w', encoding='utf-8') as f:
                            f.write(f"# {info['name']}ä»£ç†åˆ—è¡¨ï¼ˆåŒ…å«å¯¼å…¥çš„æˆåŠŸä»£ç†ï¼‰\n")
                            f.write(f"# æ€»ä»£ç†æ•°: {len(all_proxies)}\n")
                            f.write(f"# å¯¼å…¥æˆåŠŸä»£ç†: {len(successful_proxies)}\n")
                            f.write("#" * 50 + "\n\n")
                            
                            for proxy in sorted(all_proxies):
                                f.write(f"{proxy}\n")
                        
                        print(f"âœ… å¯¼å…¥ {len(successful_proxies)} ä¸ªæˆåŠŸ{info['name']}ä»£ç†")
                        imported_count += len(successful_proxies)
                        
                except Exception as e:
                    print(f"âŒâŒ å¯¼å…¥{info['name']}ä»£ç†å¤±è´¥: {e}")
        
        if imported_count > 0:
            print(f"âœ… æ€»å…±å¯¼å…¥ {imported_count} ä¸ªæˆåŠŸä»£ç†")
        else:
            print("â„¹â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å…¥çš„æˆåŠŸä»£ç†")
    
    def clean_proxy(self, proxy_str):
        """æ¸…ç†ä»£ç†æ ¼å¼"""
        proxy = proxy_str.strip()
        
        if '/#' in proxy:
            proxy = proxy.split('/#')[0]
        
        if proxy.startswith(('http://', 'https://', 'socks4://', 'socks5://')):
            return proxy
        
        if ':' in proxy:
            parts = proxy.split(':')
            if len(parts) == 2:
                ip = parts[0].strip()
                port = parts[1].strip()
                
                if '/' in port:
                    port = port.split('/')[0]
                
                return f"{ip}:{port}"
        
        return proxy
    
    def validate_proxy(self, proxy):
        """éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        if not proxy:
            return False
        
        if '@' in proxy:
            try:
                if '://' in proxy:
                    protocol_part, rest = proxy.split('://', 1)
                    if '@' in rest:
                        auth_part, host_part = rest.split('@', 1)
                        if ':' in host_part:
                            host, port_str = host_part.split(':', 1)
                            if '/' in port_str:
                                port_str = port_str.split('/')[0]
                            
                            try:
                                port = int(port_str)
                                return 1 <= port <= 65535
                            except ValueError:
                                return False
                return False
            except Exception:
                return False
        else:
            if ':' not in proxy:
                return False
            
            try:
                ip, port_str = proxy.split(':', 1)
                port = int(port_str)
                
                if not 1 <= port <= 65535:
                    return False
                
                try:
                    socket.inet_aton(ip)
                    return True
                except socket.error:
                    return True
                    
            except (ValueError, TypeError):
                return False
    
    def get_proxy_url(self, proxy, proxy_type):
        """æ ¹æ®ä»£ç†ç±»å‹ç”Ÿæˆä»£ç†URL"""
        if proxy.startswith(('http://', 'https://', 'socks4://', 'socks5://')):
            return proxy
        
        if proxy_type == "HTTP":
            return f"http://{proxy}"
        elif proxy_type == "HTTPS":
            return f"https://{proxy}"
        elif proxy_type == "SOCKS4":
            return f"socks4://{proxy}"
        elif proxy_type == "SOCKS5":
            return f"socks5://{proxy}"
        else:
            return f"http://{proxy}"
    
    def download_proxy_list(self, url, proxy_type):
        """ä»æŒ‡å®šURLä¸‹è½½ä»£ç†åˆ—è¡¨"""
        print(f"ğŸŒğŸŒ ä¸‹è½½{proxy_type}ä»£ç†: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            }
            
            response = requests.get(url, headers=headers, timeout=30, verify=False)
            
            if response.status_code == 200:
                proxies = []
                for line in response.text.splitlines():
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('//'):
                        proxy = self.clean_proxy(line)
                        if proxy and self.validate_proxy(proxy):
                            proxies.append(proxy)
                
                print(f"âœ… ä¸‹è½½åˆ° {len(proxies)} ä¸ª{proxy_type}ä»£ç†")
                return proxies
            else:
                print(f"âŒâŒ ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒâŒ ä¸‹è½½å¤±è´¥: {e}")
            return []
    
    def save_proxies_to_file(self, proxies, file_path, proxy_type):
        """ä¿å­˜ä»£ç†åˆ—è¡¨åˆ°æ–‡ä»¶"""
        if not proxies:
            print(f"âš ï¸ æ²¡æœ‰{proxy_type}ä»£ç†å¯ä¿å­˜")
            return
        
        unique_proxies = list(set(proxies))
        print(f"ğŸ“ŠğŸ“Š {proxy_type}ä»£ç†å»é‡å: {len(unique_proxies)} ä¸ª")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {proxy_type}ä»£ç†åˆ—è¡¨\n")
            f.write(f"# æ€»ä»£ç†æ•°: {len(unique_proxies)}\n")
            f.write("#" * 50 + "\n\n")
            
            for proxy in sorted(unique_proxies):
                f.write(f"{proxy}\n")
        
        print(f"ğŸ’¾ğŸ’¾ å·²ä¿å­˜åˆ°: {file_path}")
    
    def parse_source_file(self):
        """è§£æsource.txtæ–‡ä»¶ï¼Œæå–ä¸‹è½½é“¾æ¥"""
        source_file = os.path.join(self.base_dir, "source.txt")
        
        if not os.path.exists(source_file):
            print(f"âŒâŒ source.txtæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
            return {}
        
        print(f"ğŸ“ğŸ“ è§£æsource.txtæ–‡ä»¶")
        
        proxy_type_mapping = {
            'http': 'http',
            'https': 'https', 
            'socks4': 'socks4',
            'socks5': 'socks5'
        }
        
        all_links = {
            'http': [],
            'https': [],
            'socks4': [],
            'socks5': []
        }
        
        try:
            with open(source_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
                
                try:
                    data = json.loads(content)
                    
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict):
                                for proxy_type, urls in item.items():
                                    if proxy_type.lower() in proxy_type_mapping:
                                        mapped_type = proxy_type_mapping[proxy_type.lower()]
                                        
                                        if isinstance(urls, str):
                                            if urls.startswith('[') and urls.endswith(']'):
                                                try:
                                                    url_list = json.loads(urls)
                                                    if isinstance(url_list, list):
                                                        for url in url_list:
                                                            all_links[mapped_type].append(url)
                                                except:
                                                    all_links[mapped_type].append(urls)
                                            else:
                                                all_links[mapped_type].append(urls)
                                        elif isinstance(urls, list):
                                            for url in urls:
                                                all_links[mapped_type].append(url)
                    
                    print(f"âœ… è§£ææˆåŠŸï¼Œæ‰¾åˆ° {sum(len(links) for links in all_links.values())} ä¸ªé“¾æ¥")
                    
                except json.JSONDecodeError as e:
                    print(f"âŒâŒ JSONè§£æå¤±è´¥: {e}")
                    return {}
            
            total_links = sum(len(links) for links in all_links.values())
            print(f"ğŸ“ŠğŸ“Š æ€»å…±æ‰¾åˆ° {total_links} ä¸ªä¸‹è½½é“¾æ¥")
            
            return all_links
            
        except Exception as e:
            print(f"âŒâŒ è§£æsource.txtæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def download_and_classify_proxies(self):
        """ä»source.txtä¸‹è½½ä»£ç†å¹¶åˆ†ç±»ä¿å­˜"""
        print("\n" + "="*60)
        print("ğŸ“¥ğŸ“¥ å¼€å§‹ä¸‹è½½ä»£ç†å¹¶åˆ†ç±»")
        print("="*60)
        
        all_links = self.parse_source_file()
        
        if not any(all_links.values()):
            print("âŒâŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥")
            return
        
        total_downloaded = 0
        
        for proxy_type, links in all_links.items():
            if not links:
                continue
                
            print(f"\nğŸ“¥ğŸ“¥ å¤„ç†{proxy_type.upper()}ä»£ç†...")
            
            all_proxies = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_to_url = {
                    executor.submit(self.download_proxy_list, url, proxy_type): url 
                    for url in links
                }
                
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        proxies = future.result()
                        all_proxies.extend(proxies)
                    except Exception as e:
                        print(f"âŒâŒ ä¸‹è½½å¤±è´¥: {e}")
            
            if all_proxies:
                file_path = os.path.join(self.base_dir, self.proxy_files[proxy_type]['file'])
                self.save_proxies_to_file(all_proxies, file_path, proxy_type.upper())
                total_downloaded += len(all_proxies)
            else:
                print(f"âŒâŒ æ²¡æœ‰ä¸‹è½½åˆ°æœ‰æ•ˆçš„{proxy_type}ä»£ç†")
        
        print(f"\nğŸ“¥ğŸ“¥ ä¸‹è½½å®Œæˆï¼Œå¼€å§‹å¯¼å…¥ä¸Šæ¬¡æµ‹è¯•æˆåŠŸçš„ä»£ç†...")
        self.import_previous_successful_proxies()
        
        print(f"\nâœ… ä¸‹è½½å’Œå¯¼å…¥å®Œæˆ! æ€»å…±ä¸‹è½½ {total_downloaded} ä¸ªä»£ç†")
        return total_downloaded
    
    def load_proxies(self, file_path, limit=0):
        """ä»æ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨"""
        print(f"ğŸ“ğŸ“ åŠ è½½ä»£ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        
        if not os.path.exists(file_path):
            print(f"âŒâŒ æ–‡ä»¶ä¸å­˜åœ¨!")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                proxies = []
                lines = 0
                
                for line in f:
                    line = line.strip()
                    
                    if not line or line.startswith('#') or line.startswith('//'):
                        continue
                    
                    proxy = self.clean_proxy(line)
                    if proxy and self.validate_proxy(proxy):
                        proxies.append(proxy)
                        lines += 1
                    
                    if limit > 0 and lines >= limit:
                        break
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(proxies)} ä¸ªä»£ç†")
                if limit > 0 and lines >= limit:
                    print(f"ğŸ“ŠğŸ“Š åªåŠ è½½å‰ {limit} ä¸ªä»£ç†")
                
                return proxies
                
        except Exception as e:
            print(f"âŒâŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def test_single_url(self, proxy, test_config, proxy_type):
        """æµ‹è¯•å•ä¸ªURL - ä½¿ç”¨requestså†…ç½®çš„SOCKSæ”¯æŒ"""
        result = {
            'proxy': proxy,
            'proxy_type': proxy_type,
            'test_name': test_config['name'],
            'test_url': test_config['url'],
            'success': False,
            'latency_ms': 0,
            'status_code': 0,
            'error': None,
            'timestamp': datetime.now().strftime("%H:%M:%S"),
            'site_abbr': test_config.get('site_abbr', 'web')
        }
        
        # ç”Ÿæˆä»£ç†URL
        proxy_url = self.get_proxy_url(proxy, proxy_type)
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        
        try:
            start_time = time.time()
            
            # ä½¿ç”¨requestså†…ç½®çš„ä»£ç†æ”¯æŒ
            response = requests.get(
                test_config['url'],
                proxies=proxies,
                timeout=test_config.get('timeout', 8),
                headers=headers,
                verify=False,
                allow_redirects=True
            )
            
            latency = time.time() - start_time
            result['latency_ms'] = latency * 1000
            result['status_code'] = response.status_code
            
            if response.status_code == 200:
                check_string = test_config.get('check_string', '')
                if check_string:
                    if check_string.lower() in response.text.lower():
                        result['success'] = True
                        result['error'] = None
                    else:
                        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«å¸¸è§HTMLæ ‡è®°
                        page_text = response.text.lower()
                        common_indicators = ['html', 'http', 'www', 'com', 'net', 'org', 'title', 'body', 'origin']
                        indicators_found = sum(1 for indicator in common_indicators if indicator in page_text)
                        
                        if indicators_found >= 1:
                            result['success'] = True
                            result['error'] = f"æœªæ‰¾åˆ° '{check_string}' ä½†é¡µé¢æœ‰æ•ˆ"
                        else:
                            result['error'] = f"æœªæ‰¾åˆ° '{check_string}' ä¸”é¡µé¢æ— æ•ˆ"
                else:
                    result['success'] = True
                    result['error'] = None
            else:
                result['error'] = f"HTTP {response.status_code}"
                
        except requests.exceptions.ConnectTimeout:
            result['error'] = 'è¿æ¥è¶…æ—¶'
        except requests.exceptions.ReadTimeout:
            result['error'] = 'è¯»å–è¶…æ—¶'
        except requests.exceptions.ConnectionError as e:
            error_str = str(e)
            if 'timed out' in error_str.lower():
                result['error'] = 'è¿æ¥è¶…æ—¶'
            elif 'reset' in error_str.lower():
                result['error'] = 'è¿æ¥è¢«é‡ç½®'
            elif 'refused' in error_str.lower():
                result['error'] = 'è¿æ¥è¢«æ‹’ç»'
            else:
                result['error'] = f'è¿æ¥é”™è¯¯: {error_str[:30]}'
        except requests.exceptions.ProxyError as e:
            error_str = str(e)
            if 'timed out' in error_str.lower():
                result['error'] = 'ä»£ç†è¶…æ—¶'
            elif 'socks' in error_str.lower():
                result['error'] = 'SOCKSä»£ç†é”™è¯¯'
            else:
                result['error'] = f'ä»£ç†é”™è¯¯: {error_str[:30]}'
        except requests.exceptions.SSLError as e:
            result['error'] = f'SSLé”™è¯¯: {str(e)[:30]}'
        except socket.timeout:
            result['error'] = 'Socketè¶…æ—¶'
        except Exception as e:
            error_str = str(e)
            result['error'] = f'å…¶ä»–é”™è¯¯: {error_str[:30]}'
        
        return result
    
    def test_proxy_connectivity(self, proxy, proxy_type):
        """æµ‹è¯•å•ä¸ªä»£ç†çš„è¿é€šæ€§"""
        test_urls = self.get_test_urls()
        best_result = None
        
        for test_config in test_urls:
            result = self.test_single_url(proxy, test_config, proxy_type)
            
            if result['success']:
                return result
            
            if best_result is None or (result.get('status_code', 0) > 0 and best_result.get('status_code', 0) == 0):
                best_result = result
            
            if result.get('status_code', 0) > 0:
                break
        
        return best_result or {
            'proxy': proxy,
            'proxy_type': proxy_type,
            'success': False,
            'error': 'æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥',
            'latency_ms': 0,
            'test_name': 'ç»¼åˆæµ‹è¯•',
            'test_url': 'å¤šä¸ªURL',
            'timestamp': datetime.now().strftime("%H:%M:%S"),
            'site_abbr': 'unk'
        }
    
    def batch_test_proxies(self, proxies, proxy_type, max_workers=20):
        """æ‰¹é‡æµ‹è¯•ä»£ç†"""
        if not proxies:
            return [], []
        
        print(f"\nğŸš€ğŸš€ å¼€å§‹æµ‹è¯• {len(proxies)} ä¸ª{proxy_type}ä»£ç†")
        print(f"ğŸ“ŠğŸ“Š å¹¶å‘çº¿ç¨‹: {max_workers}")
        print(f"â±â±â± è¶…æ—¶æ—¶é—´: 8ç§’")
        print("-"*50)
        
        all_results = []
        successful_results = []
        
        def worker(proxy):
            result = self.test_proxy_connectivity(proxy, proxy_type)
            
            with self.lock:
                all_results.append(result)
                
                if result['success']:
                    successful_results.append(result)
                
                self.total_tested += 1
                if result['success']:
                    self.successful += 1
                else:
                    self.failed += 1
                
                if self.total_tested % 10 == 0 or self.total_tested == len(proxies):
                    percentage = self.total_tested / len(proxies) * 100
                    print(f"\rğŸ“ˆğŸ“ˆ è¿›åº¦: {self.total_tested}/{len(proxies)} "
                          f"[{percentage:.1f}%] | "
                          f"âœ…: {self.successful} | "
                          f"âŒâŒ: {self.failed}", end="")
            
            return result
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(worker, proxy): proxy for proxy in proxies}
            for future in concurrent.futures.as_completed(futures):
                pass
        
        total_time = time.time() - start_time
        
        print()
        print(f"â±â±â± æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ“ŠğŸ“Š å¹³å‡é€Ÿåº¦: {len(proxies)/total_time:.1f}ä¸ª/ç§’")
        
        return all_results, successful_results
    
    def display_results(self, all_results, successful_results, proxy_type):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“ŠğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        
        total = len(all_results)
        success_rate = (self.successful / total * 100) if total > 0 else 0
        
        print(f"ä»£ç†ç±»å‹: {proxy_type}")
        print(f"æ€»ä»£ç†æ•°: {total}")
        print(f"æˆåŠŸä»£ç†: {self.successful} ({success_rate:.1f}%)")
        print(f"å¤±è´¥ä»£ç†: {total - self.successful}")
        
        if successful_results:
            site_stats = {}
            for result in successful_results:
                site_abbr = result.get('site_abbr', 'unk')
                site_stats[site_abbr] = site_stats.get(site_abbr, 0) + 1
            
            print(f"\nğŸŒğŸŒ æˆåŠŸç½‘ç«™åˆ†å¸ƒ:")
            for site_abbr, count in site_stats.items():
                print(f"  {site_abbr.upper()}æˆåŠŸ: {count}ä¸ª")
        
        if successful_results:
            latencies = [r['latency_ms'] for r in successful_results]
            avg_latency = sum(latencies) / len(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            
            print(f"\nâ±â±â± å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms")
            print(f"  æœ€å¿«å»¶è¿Ÿ: {min_latency:.0f}ms")
            print(f"  æœ€æ…¢å»¶è¿Ÿ: {max_latency:.0f}ms")
            
            print(f"\nğŸ“ˆğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒ:")
            latency_ranges = [
                (0, 100, "æå¿« <100ms"),
                (100, 200, "å¿«é€Ÿ 100-200ms"),
                (200, 500, "ä¸­ç­‰ 200-500ms"),
                (500, 1000, "è¾ƒæ…¢ 500ms-1s"),
                (1000, 3000, "æ…¢ 1-3s"),
                (3000, float('inf'), "å¾ˆæ…¢ >3s")
            ]
            
            for min_r, max_r, label in latency_ranges:
                count = sum(1 for r in successful_results if min_r <= r['latency_ms'] < max_r)
                if count > 0:
                    percentage = count / len(successful_results) * 100
                    bar_length = int(percentage / 5)
                    bar = "â–ˆ" * bar_length
                    print(f"  {label:15s}: {count:3d}ä¸ª ({percentage:5.1f}%) {bar}")
        
        if successful_results:
            fastest = sorted(successful_results, key=lambda x: x['latency_ms'])[:5]
            
            print(f"\nğŸš€ğŸš€ æœ€å¿«çš„5ä¸ªä»£ç†:")
            for i, result in enumerate(fastest, 1):
                latency = result['latency_ms']
                site_abbr = result.get('site_abbr', 'unk')
                
                if latency < 100:
                    indicator = "ğŸŸ¢ğŸŸ¢ğŸŸ¢"
                elif latency < 200:
                    indicator = "ğŸŸ¡ğŸŸ¡ğŸŸ¡"
                elif latency < 500:
                    indicator = "ğŸŸ ğŸŸ ğŸŸ¡"
                elif latency < 1000:
                    indicator = "ğŸ”´ğŸ”´"
                else:
                    indicator = "âš«âš«"
                
                print(f"  {i}. {indicator} {result['proxy']:20s} | {latency:5.0f}ms | {site_abbr}")
        
        failed_results = [r for r in all_results if not r['success']]
        if failed_results:
            error_stats = {}
            for r in failed_results:
                error = r.get('error', 'æœªçŸ¥é”™è¯¯')
                error_key = error.split(':')[0] if ':' in error else error
                error_stats[error_key] = error_stats.get(error_key, 0) + 1
            
            print(f"\nâŒâŒ å¤±è´¥åŸå› åˆ†æ ({len(failed_results)} ä¸ª):")
            for error, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = count / len(failed_results) * 100
                print(f"  {error:30s}: {count:3d}ä¸ª ({percentage:5.1f}%)")
    
    def save_results(self, all_results, successful_results, proxy_type):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°resultæ–‡ä»¶å¤¹"""
        if successful_results:
            successful_sorted = sorted(successful_results, key=lambda x: x['latency_ms'])
            
            result_file = os.path.join(self.result_dir, f"{proxy_type.lower()}.txt")
            
            with open(result_file, 'w', encoding='utf-8') as f:
                f.write(f"# {proxy_type}ä»£ç†æµ‹è¯•ç»“æœ - æœ‰æ•ˆä»£ç†åˆ—è¡¨\n")
                f.write(f"# æ€»æµ‹è¯•æ•°: {len(all_results)}\n")
                f.write(f"# æˆåŠŸä»£ç†: {len(successful_results)}\n")
                f.write(f"# æˆåŠŸç‡: {len(successful_results)/len(all_results)*100:.1f}%\n")
                f.write(f"# æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ ¼å¼: åè®®://IP:ç«¯å£/#å»¶è¿Ÿms%20ç½‘ç«™ç¼©å†™\n")
                f.write("#"*60 + "\n\n")
                
                for i, result in enumerate(successful_sorted, 1):
                    proxy = result['proxy']
                    latency = result['latency_ms']
                    site_abbr = result.get('site_abbr', 'unk')
                    
                    if latency.is_integer():
                        latency_str = f"{int(latency)}ms"
                    else:
                        latency_str = f"{latency:.1f}ms"
                    
                    proxy_url = self.get_proxy_url(proxy, proxy_type)
                    f.write(f"{proxy_url}/#{latency_str}%20{site_abbr}\n")
            
            print(f"ğŸ’¾ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
            print(f"ğŸ“‹ğŸ“‹ æ ¼å¼: åè®®://IP:ç«¯å£/#å»¶è¿Ÿms%20ç½‘ç«™ç¼©å†™")
            
            return result_file
        else:
            print(f"âŒâŒ æ²¡æœ‰æœ‰æ•ˆçš„{proxy_type}ä»£ç†ï¼Œæœªä¿å­˜ç»“æœ")
            return None
    
    def test_proxy_type(self, proxy_type, max_workers=20, limit=0):
        """æµ‹è¯•ç‰¹å®šç±»å‹çš„ä»£ç†"""
        if proxy_type not in self.proxy_files:
            print(f"âŒâŒ æ— æ•ˆçš„ä»£ç†ç±»å‹: {proxy_type}")
            return
        
        info = self.proxy_files[proxy_type]
        file_path = os.path.join(self.base_dir, info['file'])
        
        if not os.path.exists(file_path):
            print(f"âŒâŒ ä»£ç†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        print(f"\n" + "="*60)
        print(f"ğŸ¯ğŸ¯ å¼€å§‹æµ‹è¯• {info['name']} ä»£ç†")
        print(f"ğŸ“ğŸ“ ä»£ç†æ–‡ä»¶: {file_path}")
        print("="*60)
        
        # é‡ç½®è®¡æ•°å™¨
        self.total_tested = 0
        self.successful = 0
        self.failed = 0
        
        # åŠ è½½ä»£ç†
        proxies = self.load_proxies(file_path, limit)
        
        if not proxies:
            print("âŒâŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»£ç†ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æµ‹è¯•ä»£ç†
        all_results, successful_results = self.batch_test_proxies(
            proxies=proxies,
            proxy_type=info['name'],
            max_workers=max_workers
        )
        
        # æ˜¾ç¤ºç»“æœ
        self.display_results(all_results, successful_results, info['name'])
        
        # ä¿å­˜ç»“æœ
        saved_file = self.save_results(all_results, successful_results, info['name'])
        
        if successful_results:
            fastest = min(successful_results, key=lambda x: x['latency_ms'])
            
            print(f"\nğŸ¯ğŸ¯ ä½¿ç”¨ç¤ºä¾‹:")
            proxy_url = self.get_proxy_url(fastest['proxy'], info['name'])
            print(f"  # åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨:")
            print(f"  curl -x {proxy_url} {fastest.get('test_url', 'https://example.com')}")
            
            print(f"\n  # åœ¨Pythonä¸­ä½¿ç”¨:")
            print(f"  import requests")
            print(f"  proxies = {{'http': '{proxy_url}', 'https': '{proxy_url}'}}")
            print(f"  response = requests.get('{fastest.get('test_url', 'https://example.com')}', proxies=proxies)")
        
        print(f"\n{'='*60}")
        print(f"âœ… {info['name']}ä»£ç†æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“…ğŸ“… å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        return len(successful_results)
    
    def auto_run(self):
        """è‡ªåŠ¨è¿è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
        print("ğŸš€ğŸš€ å¼€å§‹GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•")
        print(f"ğŸ“ğŸ“ å·¥ä½œç›®å½•: {self.base_dir}")
        print(f"ğŸ’¾ğŸ’¾ ç»“æœç›®å½•: {self.result_dir}")
        print("="*60)
        
        start_time = time.time()
        
        # 1. ä¸‹è½½ä»£ç†
        print("\nğŸ“¥ğŸ“¥ æ­¥éª¤1: ä¸‹è½½ä»£ç†")
        downloaded_count = self.download_and_classify_proxies()
        
        # 2. å¯¼å…¥ä¸Šæ¬¡æˆåŠŸçš„ä»£ç†
        print("\nğŸ“¥ğŸ“¥ æ­¥éª¤2: å¯¼å…¥ä¸Šæ¬¡æˆåŠŸçš„ä»£ç†")
        self.import_previous_successful_proxies()
        
        # 3. ä¾æ¬¡æµ‹è¯•å„ç§ç±»å‹çš„ä»£ç†
        print("\nğŸ§ªğŸ§ª æ­¥éª¤3: å¼€å§‹æµ‹è¯•ä»£ç†")
        
        test_results = {}
        proxy_types = ['http', 'https', 'socks4', 'socks5']
        
        for proxy_type in proxy_types:
            if proxy_type in self.proxy_files:
                info = self.proxy_files[proxy_type]
                file_path = os.path.join(self.base_dir, info['file'])
                
                if os.path.exists(file_path):
                    print(f"\n" + "="*60)
                    print(f"ğŸ§ªğŸ§ª æµ‹è¯• {info['name']} ä»£ç†")
                    print("="*60)
                    
                    successful_count = self.test_proxy_type(proxy_type, max_workers=20, limit=0)
                    test_results[proxy_type] = successful_count
                    
                    # çŸ­æš‚æš‚åœï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
                    time.sleep(2)
                else:
                    print(f"âŒâŒ è·³è¿‡{info['name']}ä»£ç†æµ‹è¯•ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    test_results[proxy_type] = 0
            else:
                print(f"âŒâŒ æœªçŸ¥ä»£ç†ç±»å‹: {proxy_type}")
                test_results[proxy_type] = 0
        
        # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“ŠğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        total_successful = sum(test_results.values())
        print(f"ğŸ“ˆğŸ“ˆ æ€»æˆåŠŸä»£ç†æ•°: {total_successful}")
        
        for proxy_type, count in test_results.items():
            if proxy_type in self.proxy_files:
                info = self.proxy_files[proxy_type]
                print(f"  {info['name']}: {count} ä¸ª")
        
        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        
        print(f"\nâ±â±â± æ€»è€—æ—¶: {minutes}åˆ†{seconds}ç§’")
        print(f"ğŸ’¾ğŸ’¾ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {self.result_dir}")
        
        # ç”ŸæˆREADMEæ–‡ä»¶
        self.generate_readme(test_results, total_time)
        
        print("\nâœ… GitHubè‡ªåŠ¨ä»£ç†æµ‹è¯•å®Œæˆ!")
    
    def generate_readme(self, test_results, total_time):
        """ç”ŸæˆREADMEæ–‡ä»¶"""
        readme_file = os.path.join(self.base_dir, "README.md")
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write("# ğŸ”ğŸ” ä»£ç†æµ‹è¯•ä»“åº“\n\n")
            f.write("è‡ªåŠ¨æµ‹è¯•å’ŒéªŒè¯HTTPã€HTTPSã€SOCKS4ã€SOCKS5ä»£ç†\n\n")
            
            f.write("## ğŸ“ŠğŸ“Š æœ€æ–°æµ‹è¯•ç»“æœ\n\n")
            f.write(f"**æœ€åæ›´æ–°:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("| ä»£ç†ç±»å‹ | æˆåŠŸæ•°é‡ | æµ‹è¯•æ—¶é—´ |\n")
            f.write("|---------|---------|---------|\n")
            
            for proxy_type, count in test_results.items():
                if proxy_type in self.proxy_files:
                    info = self.proxy_files[proxy_type]
                    f.write(f"| {info['name']} | {count} | {datetime.now().strftime('%H:%M:%S')} |\n")
            
            minutes = int(total_time // 60)
            seconds = int(total_time % 60)
            f.write(f"| **æ€»è®¡** | **{sum(test_results.values())}** | **{minutes}åˆ†{seconds}ç§’** |\n\n")
            
            f.write("## ğŸ“ğŸ“ æ–‡ä»¶è¯´æ˜\n\n")
            f.write("- `proxy_tester.py` - ä¸»æµ‹è¯•è„šæœ¬\n")
            f.write("- `source.txt` - ä»£ç†æºé…ç½®\n")
            f.write("- `ym.txt` - æµ‹è¯•ç½‘ç«™åˆ—è¡¨\n")
            f.write("- `http.txt` - HTTPä»£ç†åˆ—è¡¨\n")
            f.write("- `https.txt` - HTTPSä»£ç†åˆ—è¡¨\n")
            f.write("- `sock4.txt` - SOCKS4ä»£ç†åˆ—è¡¨\n")
            f.write("- `sock5.txt` - SOCKS5ä»£ç†åˆ—è¡¨\n")
            f.write("- `result/` - æµ‹è¯•ç»“æœç›®å½•\n\n")
            
            f.write("## ğŸš€ğŸš€ğŸš€ ä½¿ç”¨æ–¹æ³•\n\n")
            f.write("### è‡ªåŠ¨è¿è¡Œ\n")
            f.write("```bash\npython proxy_tester.py\n```\n\n")
            
            f.write("### æ‰‹åŠ¨è¿è¡Œ\n")
            f.write("```python\nfrom proxy_tester import GitHubProxyTester\n")
            f.write("tester = GitHubProxyTester()\n")
            f.write("tester.auto_run()  # è‡ªåŠ¨è¿è¡Œå®Œæ•´æµç¨‹\n")
            f.write("```\n\n")
            
            f.write("## âš™âš™âš™ï¸ é…ç½®è¯´æ˜\n\n")
            f.write("### source.txt æ ¼å¼\n")
            f.write("```json\n")
            f.write("[\n")
            f.write('  {"http": ["http://example.com/proxy.txt", "http://example2.com/proxy.txt"]},\n')
            f.write('  {"https": ["https://example.com/https.txt"]},\n')
            f.write('  {"socks4": ["http://example.com/socks4.txt"]},\n')
            f.write('  {"socks5": ["http://example.com/socks5.txt"]}\n')
            f.write("]\n")
            f.write("```\n\n")
            
            f.write("### ym.txt æ ¼å¼\n")
            f.write("```\n")
            f.write("# æ¯è¡Œä¸€ä¸ªæµ‹è¯•ç½‘ç«™\n")
            f.write("https://www.google.com\n")
            f.write("https://www.bing.com\n")
            f.write("https://telegram.org\n")
            f.write("```\n\n")
            
            f.write("## ğŸ“„ğŸ“„ è®¸å¯è¯\n")
            f.write("MIT License\n")
        
        print(f"ğŸ“„ğŸ“„ å·²ç”ŸæˆREADMEæ–‡ä»¶: {readme_file}")

def main():
    """ä¸»å‡½æ•°"""
    tester = GitHubProxyTester()
    
    try:
        # è‡ªåŠ¨è¿è¡Œå®Œæ•´æµç¨‹
        tester.auto_run()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
